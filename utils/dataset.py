import numpy as np


class Dataset:
    """Simple input, output tied dataset.
    Attributes:
        inputs: np.ndarray, input tensor.
        outputs: np.ndarray, output tensor.
        ndata: int, size of the inputs and outputs.
        batch_size: int, size of the batch.
        num_sets: int, number of the iteration.
    """
    def __init__(self, inputs, outputs, batch_size=None):
        self.inputs = inputs
        self.outputs = outputs

        # size of the inputs and outpus should be same
        self.ndata = len(self.inputs)
        assert self.ndata == len(self.outputs)

        if batch_size is None:
            self.batch_size = self.ndata
        else:
            self.batch_size = min(batch_size, self.ndata)

        self.num_sets = self.ndata // self.batch_size

    def __iter__(self):
        self.idx = 0
        return self

    def __next__(self):
        if self.idx >= self.num_sets:
            raise StopIteration()

        start = self.idx * self.batch_size
        end = (self.idx + 1) * self.batch_size

        # sample data
        inputs = self.inputs[start:end]
        outputs = self.outputs[start:end]

        self.idx += 1
        return inputs, outputs


class BiasedDataset:
    """Dataset API for biased data, a case in which one label is far more than the other.
    Attributes:
        malware: np.ndarray, malware data.
        normal: np.ndarray, normal software data.
        batch_size: int, size of the batch.
        shuffle: bool, whether shuffle data or not.
        in_batch: int, small batch for each side.
        length: int, longer length between malware data and normal data.
        num_sets: int, number of iterations.
        malware_dataset: Rotator, rotatable dataset wrapper for malware data.
        normal_dataset: Rotator, rotatable dataset wrapper for normal data.
    """
    def __init__(self, malware, normal, batch_size, shuffle=False):
        self.malware = malware
        self.normal = normal
        self.batch_size = batch_size
        self.shuffle = shuffle

        self.in_batch = batch_size // 2
        # take longer one
        self.length = max(len(self.malware), len(self.normal))

        self.num_sets = self.length // self.in_batch
        if self.length % self.in_batch != 0:
            self.num_sets += 1

        # dataset wrapper
        self.malware_dataset = BiasedDataset.Rotator(self.malware, self.in_batch)
        self.normal_dataset = BiasedDataset.Rotator(self.normal, self.in_batch)

    def __iter__(self):
        # prepare iterator
        self.idx = 0
        self.malware_iter = iter(self.malware_dataset)
        self.normal_iter = iter(self.normal_dataset)
        return self

    def __next__(self):
        if self.idx >= self.num_sets:
            raise StopIteration()

        malware = next(self.malware_iter)
        normal = next(self.normal_iter)

        # concatenate data
        dataset = np.concatenate([malware, normal], axis=0)
        # prepare label
        label = np.array([1] * self.in_batch + [0] * self.in_batch, dtype=np.int32)

        if self.shuffle:
            # shuffle dataset and label
            indices = np.random.permutation(self.in_batch * 2)
            dataset, label = dataset[indices], label[indices]

        self.idx += 1
        return dataset, label

    class Rotator:
        """Rotatable dataset for infinite iteration.
        Attributes:
            dataset: np.ndarray, single dataset.
            batch_size: int, size of the batch.
            num_sets: int, number of iterations for one epoch.
            rest: int, rest of the data.
        """
        def __init__(self, dataset, batch_size):
            self.dataset = dataset
            self.batch_size = batch_size

            self.num_sets = len(self.dataset) // self.batch_size
            self.rest = len(self.dataset) % self.batch_size

        def __iter__(self):
            self.idx = 0
            self.one_more = self.rest != 0
            return self
        
        def __next__(self):
            # if end of the base iteration
            if self.idx >= self.num_sets:
                # if rest data exist
                if self.one_more:
                    self.one_more = False
                    # random sampling
                    sampled = np.random.choice(self.dataset, self.batch_size - self.rest)
                    res = np.concatenate([self.dataset[-self.rest:], sampled], axis=0)
                    return res
                else:
                    # reinitialize iterator
                    self.idx = 0
                    self.one_more = self.rest != 0

            # sample data
            start = self.idx * self.batch_size
            end = (self.idx + 1) * self.batch_size
            self.idx += 1
            return self.dataset[start:end]
