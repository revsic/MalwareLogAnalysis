import argparse
import os
import shutil
from random import shuffle


def init_lib(lib_name):
    """Initialize library.
    Args:
        lib_name: str, filename of the library list.
    Returns:
        Dict[str, int], list and id mapping of library names.
    """
    # open file
    with open(lib_name) as f:
        functions = f.read()
        # split with new line
        splitted = functions.split('\n')
    # make library disctionary
    lib = dict(zip(splitted, range(1, len(splitted) + 1)))
    return lib


def raw_process(log):
    """Log level log processor.
    Args:
        log: str, raw log.
    Returns:
        List[str], list of the branch data.
    """
    # replace null
    log = log.replace('\x00', '')
    # replace carriage return
    nline = log.replace('\r', '').split('\n')
    # filtering branch data
    branch = filter(lambda x: '+' in x, nline)
    return list(branch)


def api_parser(rows):
    """Parse API list.
    Args:
        rows: List[str], list of the branch data.
    Returns:
        List[str], API list.
    """
    # split with seperator
    splitted = map(lambda x: x[1:].split(','), rows)
    # filtering unmissed data
    api_filtering = filter(lambda x: len(x) == 4, splitted)
    # get name of the win32api
    apis = map(lambda x: x[3], api_filtering)
    # filtering only known api
    filt = filter(lambda x: x != '', apis)

    return list(filt)


def map_index(rows, lib):
    """Mapping name to the library index.
    Args:
        rows: List[str], name list of the library.
        lib: Dict[str, int], library indices.
    Returns:
        List[int], mapped library data.
    """
    mapped = map(lambda x: lib[x] if x in lib else 0, rows)
    return list(mapped)


def pattern_reducer(apis, max_len, min_len):
    """Reduce the api list with certain pattern.
    Args:
        apis: List[int], mapped api list.
        max_len: int, maximum length of the list.
        min_len: int, minimum length of the list.
    Returns:
        List[int], reduced api list.
    """
    # pattern size
    size = 1
    while size < len(apis) / 2:
        # start point
        ptr = 0
        while ptr < len(apis) - size * 2:
            is_pattern = True
            # pattern detection
            n = 0
            while n < size:
                if apis[ptr + n] != apis[ptr + size + n]:
                    is_pattern = False
                    break
                n += 1

            if is_pattern:
                end = ptr + size + size
                # multiple pattern detection
                len_apis = len(apis)
                while is_pattern and end + size < len_apis:
                    n = 0
                    while n < size:
                        if apis[ptr + n] != apis[end + n]:
                            is_pattern = False
                            break
                        n += 1
                    end += size

                if not is_pattern:
                    end -= size
                # remove pattern
                del apis[ptr + size:end]
                # insert info about pattern deletion
                apis.insert(ptr + size, -1)
            ptr += 1
        size += 1
    # remove pattern deletion info
    apis = list(filter(lambda x: x != -1, apis))
    # api length test
    if len(apis) > min_len:
        if len(apis) > max_len:
            apis = apis[:max_len]

        return apis
    else:
        raise ValueError('Log length is too short.')


def preprocess(name,
               label,
               max_len=100,
               min_len=5,
               lib=None,
               lib_name='./log/function_list.txt'):
    """Preprocess data.
    Args:
        name: str, filename of the log.
        label: int, label of the dataset.
        max_len: int, maximum length of the api list.
        min_len: int, minimum length of the api list.
        lib: Dict[str, int] | None, map of library name and index.
        lib_name: str, filename of the library list.
    Returns:
        List[int], preprocessed api list.
    """
    # initialize library map.
    if lib is None:
        lib = init_lib(lib_name)

    # open log file
    with open(name) as f:
        log = f.read()
        # raw process
        data = raw_process(log)
        # parse api list
        parse = api_parser(data)

        if parse != '':
            # map index
            indexed = map_index(parse, lib)
            # reduce pattern
            reduced = pattern_reducer(indexed, max_len, min_len)
            # add label
            processed = [label] + reduced

    return processed


def proc_dir(dir_name,
             label,
             max_len=100,
             min_len=5,
             lib=None,
             lib_name='./log/function_list.txt'):
    """Process all log files in given directory.
    Args:
        dir_name: str, name of the directory.
        label: int, label of the dataset.
        max_len: int, maximum length of the api list.
        min_len: int, minimum length of the api list.
        lib: Dict[str, int] | None, map of library name and index.
        lib_name: str, filename of the library list.
    """
    if lib is None:
        lib = init_lib(lib_name)
    # get list of the log files.
    sample_name = os.listdir(dir_name)
    samples = []

    for name in sample_name:
        try:
            log_name = os.path.join(dir_name, name)
            # preprocess data
            data = preprocess(log_name, label, max_len, min_len, lib)
            # append data
            samples.append(data)
        except ValueError:
            pass

    return samples


def make_csv(mal_size=450,
             norm_size=20,
             max_len=100,
             min_len=5,
             mal_dir='./log/malware',
             norm_dir='./log/normal',
             save_dir='./data',
             mal_name='mal_trainset.csv',
             norm_name='norm_trainset.csv',
             set_name='testset.csv',
             lib=None,
             lib_name='./log/function_list.txt'):
    """Make branch data to classification dataset.
    Args:
        mal_size: int, size of the malware trainset.
        norm_size: int, size of the normal software trainset.
        max_len: int, maximum length the api list.
        min_len: int, minimum length the api list.
        mal_dir: str, directory name of the malware log files.
        norm_dir: str, directory name of the normal software log files.
        save_dir: str, directory name of the result files.
        mal_name: str, dataset name of the malware trainset.
        norm_name: str, dataset name of the normal software trainset.
        set_name: str, dataset name of the testset.
        lib: Dict[str, int] | None, map of library name and index.
        lib_name: str, filename of the library list.
    """
    if lib is None:
        lib = init_lib(lib_name)

    # preprocess directory.
    malware_samples = proc_dir(mal_dir, 1, max_len, min_len, lib)
    normal_samples = proc_dir(norm_dir, 0, max_len, min_len, lib)

    # map integer dataset to strings
    malware_set = list(map(lambda x: list(map(str, x)), malware_samples))
    normal_set = list(map(lambda x: list(map(str, x)), normal_samples))

    # shuffle dataset
    shuffle(malware_set)
    shuffle(normal_set)

    def comma_binder(x):
        return ','.join(x)

    # write malware trainset
    with open(os.path.join(save_dir, mal_name), 'w') as f:
        malware = map(comma_binder, malware_set[:mal_size])
        trainset = '\n'.join(malware)
        f.write(trainset)

    # write normal software trainset
    with open(os.path.join(save_dir, norm_name), 'w') as f:
        normal = map(comma_binder, normal_set[:norm_size])
        trainset = '\n'.join(normal)
        f.write(trainset)

    # write testset
    with open(os.path.join(save_dir, set_name), 'w') as f:
        malware = map(comma_binder, malware_set[mal_size:])
        normal = map(comma_binder, normal_set[norm_size:])
        testset = '\n'.join(malware) + '\n' + '\n'.join(normal)

        f.write(testset)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--mal_size', type=int, default=450)
    parser.add_argument('--norm_size', type=int, default=20)
    parser.add_argument('--max_len', type=int, default=100)
    parser.add_argument('--min_len', type=int, default=5)
    parser.add_argument('--mal_dir', default='./log/malware')
    parser.add_argument('--norm_dir', default='./log/normal')
    parser.add_argument('--save_dir', default='./data')
    parser.add_argument('--mal_name', default='mal_trainset.csv')
    parser.add_argument('--norm_name', default='norm_trainset.csv')
    parser.add_argument('--set_name', default='testset.csv')
    parser.add_argument('--lib_name', default='./log/function_list.txt')
    args = parser.parse_args()

    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)

    make_csv(args.mal_size,
             args.norm_size,
             args.max_len,
             args.min_len,
             args.mal_dir,
             args.norm_dir,
             args.save_dir,
             args.mal_name,
             args.norm_name,
             args.set_name,
             lib_name=args.lib_name)
