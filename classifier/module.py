import tensorflow as tf


class ScaledDotProdAttn(tf.keras.Model):
    """Scaled dot-product attention layer.
    """
    def __init__(self):
        super(ScaledDotProdAttn, self).__init__()
    
    def call(self, key, query, value):
        dk = tf.shape(key)[-1]
        scale = tf.sqrt(tf.cast(dk, tf.float32))
    
        unnorm_weights = query @ tf.transpose(key, [0, 2, 1])
        weights = tf.nn.softmax(unnorm_weights / scale)

        attn = weights @ value
        return weights, attn


class SelfAttention(tf.keras.Model):
    """Self-attention layer with scaled dot-product.
    Attributes:
        key_proj: tf.keras.Model | None, (optional) key projection layer.
        query_proj: tf.keras.Model | None, (optional) query projection layer.
        attention: ScaledDotProdAttn, scaled dot-product attention layer.
    """
    def __init__(self, key_proj=None, query_proj=None):
        super(SelfAttention, self).__init__()
        self.key_proj = key_proj
        self.query_proj = query_proj
        self.attention = ScaledDotProdAttn()

    def call(self, inputs):
        key = inputs
        if self.key_proj is not None:
            key = self.key_proj(key)

        query = inputs
        if self.query_proj is not None:
            query = self.query_proj(query)

        self.weight_matrix, attn = self.attention(key, query, inputs)
        return attn


class Classifier(tf.keras.Model):
    """Classification model for malware detection.
    Attributes:
        embedding_size: int, size of the embedding input.
        attn: List[SelfAttention], self-attention layers.
        proj: List[tf.keras.layers.Dense], projection layers.
        model: tf.keras.Model, sequential model.
    """
    def __init__(self, embedding_size):
        super(Classifier, self).__init__()
        self.embedding_size = embedding_size
        self.embedding = tf.keras.layers.Embedding(self.embedding_size, 500)

        dense = lambda n: tf.keras.layers.Dense(n, activation=tf.nn.relu)
        self.attn = [
            SelfAttention(dense(128), dense(128)),
            SelfAttention(dense(64), dense(64)),
            SelfAttention(dense(32), dense(32))
        ]

        self.proj = [dense(128), dense(64), dense(32)]
        self.last_dense = tf.keras.layers.Dense(1)
        self.flatten = tf.keras.layers.Flatten()

    def call(self, input_layer):
        layer = self.embedding(input_layer)
        for attn, proj in zip(self.attn, self.proj):
            layer = attn(layer)
            layer = proj(layer)
        
        layer = self.last_dense(layer)
        layer = self.flatten(layer)
        return tf.reduce_mean(layer, axis=-1, keepdims=True)
