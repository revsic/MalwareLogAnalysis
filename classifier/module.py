import tensorflow as tf


class ScaledDotProdAttn(tf.keras.Model):
    def __init__(self):
        super(ScaledDotProdAttn, self).__init__()
    
    def call(key, query, value):
        dk = tf.shape(key)[-1]
        scale = tf.sqrt(tf.cast(dk, tf.float32))
    
        unnorm_weights = query @ tf.transpose(key, [0, 2, 1])
        weights = tf.nn.softmax(unnorm_weights / scale)

        attn = weights @ value
        return weights, attn


class SelfAttention(tf.keras.Model):
    def __init__(self, value_model, key_proj=None, query_proj=None):
        super(SelfAttention, self).__init__()
        self.value_model = value_model
        self.key_proj = key_proj
        self.query_proj = query_proj

        self.attention = ScaledDotProdAttn()

        # build model
        self.weights, self.model = self.build()

    def build(self):
        inputs = tf.keras.Input(shape=(None, None))

        key = inputs
        if self.key_proj is not None:
            key = self.key_proj(key)

        query = inputs
        if self.query_proj is not None:
            query = self.query_proj(query)

        value = self.value_model(inputs)
        weights, attn = self.attention(key, query, value)

        weights = tf.keras.layers.Model(inputs, weights)
        attn = tf.keras.layers.Model(inputs, attn)

        return weights, attn

    def call(self, inputs):
        return self.model(inputs)


class DenseOnly(tf.keras.Model):
    def __init__(self, input_layers, activation=tf.nn.relu):
        super(DenseOnly, self).__init__()
        self.input_layers = input_layers
        self.activation = activation

        self.model = tf.keras.Sequential()
        for units in self.input_layers:
            self.model.add(tf.keras.layers.Dense(units, activation=self.activation))

    def call(self, inputs):
        return self.model(inputs)


class Classifier(tf.keras.Model):
    """Classificatioin model for malware detection.
    Attributes:
        embedding_size: int, size of the embedding input.
        model: tf.keras.Model, sequential model.
    """
    def __init__(self, embedding_size):
        super(Classifier, self).__init__()
        self.embedding_size = embedding_size

        self.attn = [
            SelfAttention(DenseOnly([128, 128]), DenseOnly([128, 128]), DenseOnly([128, 128])),
            SelfAttention(DenseOnly([64, 64]), DenseOnly([64, 64]), DenseOnly([64, 64])),
            SelfAttention(DenseOnly([32, 32]), DenseOnly([32, 32]), DenseOnly([32, 32])),
        ]

        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Embedding(self.embedding_size, 500)) # [batch, timestep, 500]
        for attn in self.attn:
            self.model.add(attn)
        
        self.model.add(tf.keras.layers.Dense(1))

    def call(self, input_layer):
        return self.model(input_layer)
