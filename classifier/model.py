import os
import time

import numpy as np
import tensorflow as tf


class Model(tf.keras.Model):
    """Classificatioin model for malware detection.
    Attributes:
        embedding_size: int, size of the embedding input.
        model: tf.keras.Model, sequential model.
    """
    def __init__(self, embedding_size):
        super(Model, self).__init__()
        self.embedding_size = embedding_size

        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Embedding(self.embedding_size, 500)) # [batch, timestep, 500]
        self.model.add(tf.keras.layers.LSTM(256))                           # [batch, 256]
        self.model.add(tf.keras.layers.Dense(1))                            # [batch, 1]

    def call(self, input_layer, training=True):
        return self.model(input_layer)


class MalwareDetector:
    """Malware detection framework.
    Attributes:
        seqlen: int, length of the sequence.
        embedding_size, size of the embedding input.
        model: Model, malware classification model.
        sess: tf.Session, tensorflow session.
        inputs: tf.placeholder, input sequence.
        output: tf.placeholder, output label.
        logits: tf.Tensor, predicted probability.
        pred: tf.Tensor, predicted label.
        loss: tf.Tensor, loss object.
        opt: tf.train.AdamOptimizer, adam optimizer for optimizing loss.
        metric: tf.Tensor, accuracy tensor.
        sloss: tf.placeholder, placeholder for loss summary.
        smetric: tf.placeholder, placeholder for metric summary.
        summary: tf.summary, tensorboard summary for loss and metric.
        checkpoint: tf.train.Saver, checkpoint saver.
    """
    def __init__(self, seqlen, embedding_size, sess=None):
        self.seqlen = seqlen
        self.embedding_size = embedding_size
        self.model = Model(self.embedding_size)
        self.sess = tf.compat.v1.Session() if sess is None else sess

        self.inputs = tf.compat.v1.placeholder(tf.int32, [None, self.seqlen])
        self.output = tf.compat.v1.placeholder(tf.int32, [None])

        self.logits = self.model(self.inputs)
        self.pred = tf.cast(self.logits > 0, tf.int32)

        loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
        self.loss = loss_object(self.output[..., None], self.logits)

        self.opt = tf.compat.v1.train.AdamOptimizer(1e-2).minimize(self.loss)

        correct = tf.equal(self.pred, self.output[..., None])
        self.metric = tf.reduce_mean(tf.cast(correct, tf.float32))

        self.sloss = tf.compat.v1.placeholder(tf.float32)
        self.smetric = tf.compat.v1.placeholder(tf.float32)
        self.summary = tf.compat.v1.summary.merge([
            tf.compat.v1.summary.scalar('loss', self.sloss),
            tf.compat.v1.summary.scalar('accuracy', self.smetric)
        ])

        self.checkpoint = tf.compat.v1.train.Saver()
        self.sess.run(tf.compat.v1.global_variables_initializer())
    
    def save(self, ckpt_dir, step=None):
        """Save model to given checkpoint directory.
        Args:
            ckpt_dir: str, file path of the checkpoint.
            step: int, global step.
        """
        self.checkpoint.save(self.sess, ckpt_dir, global_step=step)
    
    def restore(self, ckpt_dir='./ckpt'):
        """Restore model from given checkpoint directory.
        Args:
            ckpt_dir: str, file path of the checkpoint.
        """
        path = tf.train.latest_checkpoint(ckpt_dir)
        self.checkpoint.restore(self.sess, path)

    def inference(self, inputs):
        """Predict wheter given branch sequence is malware or not.
        Args:
            inputs: np.ndarray, input sequence.
        Returns:
            np.ndarray, predicted label, 1 for malware, 0 for normal.
        """
        return self.sess.run(self.pred, feed_dict={self.inputs: inputs})

    def train(self,
              trainset,
              valset,
              epochs,
              ckpt_interval,
              ckpt_dir='./ckpt',
              summary_dir='./summary',
              name='default'):
        """Train malware detection model.
        Args:
            trainset: Iterable[(np.ndarray, np.ndarray)], iterable objects of training dataset.
            valset: Iterable[(np.ndarray, np.ndarray)], iterable objects of validation dataset.
            epochs: int, number of epochs.
            ckpt_interval: int, checkpoint interval.
            ckpt_dir: str, name of the checkpoint directory.
            summary_dir: str, direcotry name for the tensorboard summary.
            name: str, name of the model.
        """
        # prepare directory
        if not os.path.exists(ckpt_dir):
            os.makedirs(ckpt_dir)

        ckpt_dir = os.path.join(ckpt_dir, name)
        summary_dir = os.path.join(summary_dir, name)
        if not os.path.exists(summary_dir):
            os.makedirs(summary_dir)

        summary_val = tf.compat.v1.summary.FileWriter(os.path.join(summary_dir, 'val'), self.sess.graph)
        summary_train = tf.compat.v1.summary.FileWriter(os.path.join(summary_dir, 'train'), self.sess.graph)

        for epoch in range(epochs):
            start = time.time()

            # train loop
            tloss, tmetric, tpred = self.loop(trainset, self.train_step, 'train')
            self.write_summary(tloss, tmetric, epoch, summary_train)

            # validation loop
            vloss, vmetric, vpred = self.loop(valset, self.validation_step, 'val')
            self.write_summary(vloss, vmetric, epoch, summary_val)

            # write checkpoint
            if (epoch + 1) % ckpt_interval == 0:
                self.save(ckpt_dir, epoch)

            print('\r{} epoch, {} sec, train loss={:.5f}, train metric={:.5f}, val loss={:.5f}, val metric={:.5f}'.format(
                epoch + 1, time.time() - start, tloss, tmetric, vloss, vmetric))
            print('{} epoch, train pred=[{}]'.format(epoch + 1, tpred))
            print('{} epoch, valid pred=[{}]'.format(epoch + 1, vpred))

    def loop(self, dataset, step, name):
        """Generalized loop for training or validating model.
        Args:
            dataset: Iterable[(np.ndarray, np.ndarray)], iterable objects of dataset.
            step: Callable[[np.ndarray, np.ndarray], [float, float, List[int]]], step function.
            name: str, name of the section.
        Returns:
            (np.float, np.float, List[int]), mean of loss and metric, last predicted label.
        """
        losses, metrics = [], []
        for idx, datum in enumerate(dataset):
            loss, metric, pred = step(*datum)
            print('\r{}: {}, loss={:.5f}, metric={:.5f}'.format(name, idx, loss, metric), end='')

            losses.append(loss)
            metrics.append(metric)

        pred = ', '.join(str(p) for p in pred.reshape(-1))
        return np.mean(losses), np.mean(metric), pred

    def train_step(self, inputs, outputs):
        """Training step.
        Args:
            inputs: np.ndarray, input sequence.
            outputs: np.ndarray, output label.
        Returns:
            (np.float, np.float, List[int]), loss, metric and predicted label.
        """
        _, loss, metric, pred = self.sess.run((self.opt, self.loss, self.metric, self.pred),
                                              feed_dict={self.inputs: inputs, self.output: outputs})
        return loss, metric, pred

    def validation_step(self, inputs, outputs):
        """Validation step.
        Args:
            inputs: np.ndarray, input sequence.
            outputs: np.ndarray, output label.
        Returns:
            (np.float, np.float, List[int]), loss, metric and predicted label.
        """
        loss, metric, pred = self.sess.run((self.loss, self.metric, self.pred),
                                           feed_dict={self.inputs: inputs, self.output: outputs})
        return loss, metric, pred

    def write_summary(self, loss, metric, epoch, writer):
        """Write tensorboard summary.
        Args:
            loss: float, loss value.
            metric: float, metric value.
            epoch: int, current epoch.
            writer: tf.summary.FileWriter, summary writer.
        """
        res = self.sess.run(self.summary, feed_dict={self.sloss: loss, self.smetric: metric})
        writer.add_summary(res, global_step=epoch)
